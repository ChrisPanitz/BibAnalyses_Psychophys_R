#     asMat[rowI,colI] <- 2*(sum(coocMat)/2)*coocMat[rowI,colI] /
#                         (sum(coocMat[rowI,])*sum(coocMat[,colI])) # van Eck & Waltmann (2014)
#   }
# }
#
# ###
#
#
#
# sumCooc <- sum(coocMat)
# rowSumsCooc <- rowSums(coocMat)
# colSumsCooc <- colSums(coocMat)
#
# asMatNew <- 2 * sumCooc * coocMat / (outer(rowSumsCooc, colSumsCooc))
# ###
#
#
# graphDF$centralityAS <- NA
# graphDF$densityAS <- NA
# graphDF$centralityEI <- NA
# graphDF$densityEI <- NA
#
# for (clustI in 1:dim(graphDF)[1]){
#   graphDF$centralityAS[clustI] <- 10 * sum(asMat[clusterIDs == clustI, clusterIDs != clustI])
#   graphDF$centralityEI[clustI] <- 10 * sum(eiMat[clusterIDs == clustI, clusterIDs != clustI])
#   graphDF$densityAS[clustI] <- 100 * sum(asMat[clusterIDs == clustI, clusterIDs = clustI]/2) / sum(clusterIDs == clustI)
#   graphDF$densityEI[clustI] <- 100 * sum(eiMat[clusterIDs == clustI, clusterIDs = clustI]/2) / sum(clusterIDs == clustI)
#   #graphDF$centrality[clustI] <- sum(asMat[clusterIDs == clustI, clusterIDs != clustI]^2)
#   #graphDF$density[clustI] <- (sum(asMat[clusterIDs == clustI, clusterIDs == clustI]) / 2) / (sum(clusterIDs == clustI)*(sum(clusterIDs == clustI)-1)/2)
# }
#
# flextable(graphDF)
#as.data.frame(as.matrix(network_TI))
#for (rowI in 1:50){
#  for (colI in 1:50){
#
#  }
#}
fromYear <- 2004
toYear <- 2023
# packages
library(here)
library(dplyr)
library(bibliometrix)
library(flextable)
library(igraph)
library(ggplot2)
# parent folder
parentFolder <- here()
# Load the VOS data from files
vosMap <- read.table(paste0(parentFolder,"/vosFiles/keyTerms_",fromYear,"to",toYear,"_map.txt"),
header = TRUE, sep = "\t")
vosNetwork <- read.table(paste0(parentFolder,"/vosFiles/keyTerms_",fromYear,"to",toYear,"_network.txt"),
header = FALSE, sep = "\t")
clustCol <- read.table(paste0(parentFolder,"/vosFiles/vosColors.txt"),
header = TRUE, sep = "\t")
# rename variables
names(vosNetwork) <- c("node1", "node2", "cooccurrences")
# Create igraph object from vos data
keyGraph <- graph_from_data_frame(vosNetwork, directed = FALSE)
# add cluster and label information
V(keyGraph)$cluster <- vosMap$cluster[match(V(keyGraph)$name, vosMap$id)]
V(keyGraph)$label <- vosMap$label[match(V(keyGraph)$name, vosMap$id)]
#V(keyGraph)$label
#V(keyGraph)$cluster
graphDF <- summarize(group_by(vosMap, by = cluster), terms = paste(label, collapse = ", "))
names(graphDF) <- c("cluster", "keyterms")
graphDF$cluster <- factor(graphDF$cluster)
graphDF$clustSize <- as.vector(table(V(keyGraph)$cluster))
#graphDF$density <- NA
graphDF$keyterms <- gsub("-", " ", graphDF$keyterms)
### COMPUTE centrality and density based on bibliometrix co-occurence matrix
# # load  bibliometric data frame
# filename <- paste0(parentFolder, "/rawData/allPapers.rds")
# df <- readRDS(filename)
#
# # load multi-word key terms to keep
# terms2keep_TI <- read.table(paste0(parentFolder, "/terms/compoundTerms_TI_selected.txt"), sep = ";")[,1]
#
# # load prepared lists of terms to delete and synonyms
# terms2delete <- read.table(paste0(parentFolder, "/terms/terms2delete_TI.txt"), sep = ";")[,1]
# synonyms <- read.table(paste0(parentFolder, "/terms/synonyms_TI.txt"), sep = ";")[,1]
#
# # extract keywords from article titles
# df <- termExtraction(df, Field = "TI", ngrams = 1, remove.numbers = FALSE,
#                      remove.terms = terms2delete, synonyms = synonyms, verbose = TRUE)
#
# # custom way to treat multi-word terms as one term, words joint by hyphen
# replace2keep_TI <- gsub(" ",";",terms2keep_TI)
# replaceWith_TI <- gsub(" ","-",terms2keep_TI)
# for (i in 1:length(terms2keep_TI)){
#   df$TI_TM <- gsub(replace2keep_TI[i],replaceWith_TI[i],df$TI_TM)
# }
#
# # IND DIFF
# df$TI_TM <- gsub("INDIVIDUAL;DIFFERENCE","INDIVIDUAL-DIFFERENCE",df$TI_TM)
# df$TI_TM <- gsub("DIFFERENCE;INDIVIDUAL","INDIVIDUAL-DIFFERENCE",df$TI_TM)
#
# # extract occurrence matrix (binary; publication x keyword)
# occMat <- cocMatrix(
#   df[is.element(df$PY,fromYear:toYear),],
#   Field = "TI_TM",
#   type = "matrix",
#   n = length(V(keyGraph)$cluster),
#   sep = ";",
#   binary = FALSE,
#   short = TRUE,
#   remove.terms = terms2delete,
#   synonyms = synonyms
# )
###
# compute co-occurrence matrix (count data; keyword x keyword)
#occMat[occMat > 0] <- 1 # some terms might have been double-counted because of synonyms
#coocMat <- t(occMat) %*% occMat
# compute total occurrences of key terms
#totalOcc <- diag(coocMat)
# set diagonal (occurrences to zero)
#coocMat <- coocMat * (1-diag(dim(coocMat)[1]))
# square co-occurrences (as is done when exporting bibliometrix data to VOS viewer)
#coocMatSquared <- coocMat^2
#coocMat <- matrix(0, nrow = length(V(keyGraph)), ncol = length(V(keyGraph)))
#for (i in 1:dim(vosNetwork)[1]){
#  coocMat[vosNetwork$node1[i], vosNetwork$node2[i]] <- vosNetwork$cooccurrences[i]
#  coocMat[vosNetwork$node2[i], vosNetwork$node1[i]] <- vosNetwork$cooccurrences[i]
#}
coocMat <- as.matrix(read.table(file = paste0(parentFolder,"/vosFiles/keyTerms_",fromYear,"to",toYear,"_coocMat.txt"),
sep = ";", header = TRUE))
# compute total occurrences of key terms
totalOcc <- diag(coocMat)
# compute associative strength (van Eck & Waltmann, 2014)
#asMat <- 2 * sum(coocMatSquared)/2 * coocMatSquared /
#         (outer(rowSums(coocMatSquared), colSums(coocMatSquared)))
asMat <- 2 * sum(coocMat)/2 * coocMat /
(outer(rowSums(coocMat), colSums(coocMat)))
# count key term occurrences, compute Callon centrality and density
graphDF$totalOccurrences <- NA
graphDF$centralityAS <- NA
graphDF$densityAS <- NA
for (clustI in 1:dim(graphDF)[1]){
graphDF$totalOccurrences[clustI] <- sum(totalOcc[V(keyGraph)$cluster == clustI])
graphDF$centralityAS[clustI] <- 10 * sum(asMat[V(keyGraph)$cluster == clustI, V(keyGraph)$cluster != clustI])
graphDF$densityAS[clustI] <- 100 * sum(asMat[V(keyGraph)$cluster == clustI, V(keyGraph)$cluster == clustI]/2) /
sum(V(keyGraph)$cluster == clustI)
}
# create rank data for centrality and density; with minus because rank() ranks from lowest to highest
graphDF$centralityRank <- rank(-graphDF$centralityAS)
graphDF$densityRank <- rank(-graphDF$densityAS)
### TOP 5
createTop5 <- function(terms) {
splitTerms <- strsplit(terms, ",")[[1]] # Split the terms by commas
splitTerms <- trimws(splitTerms) # Trim whitespaces
selectedTerms <- splitTerms[1:min(5, length(splitTerms))] # Select the first 5 terms or fewer
paste(selectedTerms, collapse = "\n") # Combine them with line breaks
}
# Apply the function to graphDF
graphDF$top5 <- sapply(graphDF$keyterms, createTop5)
#graphDF[,c("centralityAS","densityAS")] <- round(graphDF[,c("centralityAS","densityAS")])
#graphDF$centralityAS <- paste0(round(centralityAS)," (",order(centralityAS, decreasing = TRUE),")")
#graphDF$densityAS <- paste0(round(densityAS)," (",order(densityAS, decreasing = TRUE),")")
clustCol$hexCol <- rgb(clustCol$red, clustCol$green, clustCol$blue, maxColorValue = 255)
dfTable <- data.frame(
cluster = graphDF$cluster,
keyterms = graphDF$keyterms,
clustSize = graphDF$clustSize,
centralityAS = paste0(round(graphDF$centralityAS)," (",graphDF$centralityRank,")"),
densityAS = paste0(round(graphDF$densityAS)," (",graphDF$densityRank,")")
)
clusterTable <- flextable(dfTable)
clusterTable <- set_header_labels(clusterTable, values = c("Cluster", "Key Terms", "Size", "Centrality", "Density"))
clusterTable <- align(clusterTable, align = "center", part  = "all")
clusterTable <- fontsize(clusterTable, size = 10, part = "all")
clusterTable <- fontsize(clusterTable, size = 10, part = "all")
clusterTable <- font(clusterTable, fontname = "Times New Roman", part = "all")
clusterTable <- bold(clusterTable, part = "header")
clusterTable <- width(clusterTable, width = 2, unit = "cm")
clusterTable <- width(clusterTable, j = 2, width = 9, unit = "cm")
for (rowI in 1:dim(dfTable)[1]){
clusterTable <- color(clusterTable, i = rowI, color = clustCol$hexCol[rowI])
}
clusterTable
# save the table
#save_as_docx(clusterTable, path = paste0(parentFolder, "/tables/keytermCoocurrences_",fromYear,"to",toYear,"_export.docx"))
save_as_docx(clusterTable, path = paste0(parentFolder, "/tables/keytermCoocurrences_",fromYear,"to",toYear,".docx"))
###
cMin <- min(graphDF$centralityRank)
cMax <- max(graphDF$centralityRank)
#cRan <- cMax - cMin
dMin <- min(graphDF$densityRank)
dMax <- max(graphDF$densityRank)
#dRan <- dMax - dMin
themMapPlotMedian <- ggplot(graphDF, aes(x = cMax+1-centralityRank, y = dMax+1-densityRank,
color = cluster, size = totalOccurrences)) +
theme_classic() +
#geom_hline(yintercept = mean(c(min(graphDF$centralityRank),max(graphDF$centralityRank))), linetype = "dashed", color = "gray50") +
#geom_vline(xintercept = mean(c(min(graphDF$densityRank),max(graphDF$densityRank))), linetype = "dashed", color = "gray50") +
geom_hline(yintercept = median(graphDF$centralityRank), linetype = "dashed", color = "gray50") +
geom_vline(xintercept = median(graphDF$densityRank), linetype = "dashed", color = "gray50") +
#annotate(geom = "text", x = 0.25, y = 7, label = "niche themes", color = "gray50") +
#annotate(geom = "text", x = 6.75, y = 7, label = "motor themes", color = "gray50") +
#annotate(geom = "text", x = 6.75, y = 0, label = "basic themes", color = "gray50") +
#annotate(geom = "text", x = 0.25, y = 0, label = "up/down themes", color = "gray50") +
geom_point(alpha = .70) +
geom_text(aes(label = top5), size = 3, fontface = "bold", color ="black") +
scale_x_continuous(name = "Centrality (Ranked)", breaks = NULL, limits = c(0,cMax+1)) +
scale_y_continuous(name = "Density (Ranked)", breaks = NULL, limits = c(0,dMax+1)) +
scale_color_manual(values = clustCol$hexCol) +
scale_size_continuous(range = c(10,30)) +
theme(
legend.position = "none"
)
themMapPlotMedian
# save median plots
ggsave(paste0(parentFolder, "/plots/thematicMaps/themMap_Median_",fromYear,"to",toYear,".pdf"), themMapPlotMedian,
width = 20, height = 20, units = "cm")
ggsave(paste0(parentFolder, "/plots/thematicMaps/themMap_Median_",fromYear,"to",toYear,".png"), themMapPlotMedian,
width = 20, height = 20, units = "cm", dpi = 600)
###
cMin <- min(graphDF$centralityAS)
cMax <- max(graphDF$centralityAS)
cRan <- cMax - cMin
dMin <- min(graphDF$densityAS)
dMax <- max(graphDF$densityAS)
dRan <- dMax - dMin
graphDF$textX <- graphDF$centralityAS
for (i in 1:(max(graphDF$centralityRank)-1)){
for (j in (i+1):max(graphDF$centralityRank)){
xDist <- (graphDF$centralityAS[i]-graphDF$centralityAS[j]) / cRan
yDist <- abs(graphDF$densityAS[i]-graphDF$densityAS[j]) / dRan
if (xDist < 0.05 & xDist > 0 & yDist < 0.05){
graphDF$textX[i] <- graphDF$textX[i] + cRan*0.07
graphDF$textX[j] <- graphDF$textX[j] - cRan*0.07
} else if (xDist > -0.05 & xDist < 0 & yDist < 0.05){
graphDF$textX[i] <- graphDF$textX[i] - cRan*0.07
graphDF$textX[j] <- graphDF$textX[j] + cRan*0.07
}
}
}
themMapPlotMean <- ggplot(graphDF, aes(x = centralityAS, y = densityAS,
color = cluster, size = totalOccurrences)) +
theme_classic() +
#geom_vline(xintercept = mean(c(min(graphDF$centralityAS),max(graphDF$centralityAS))), linetype = "dashed", color = "gray50") +
#geom_hline(yintercept = mean(c(min(graphDF$densityAS),max(graphDF$densityAS))), linetype = "dashed", color = "gray50") +
geom_vline(xintercept = mean(graphDF$centralityAS), linetype = "dashed", color = "gray50") +
geom_hline(yintercept = mean(graphDF$densityAS), linetype = "dashed", color = "gray50") +
#annotate(geom = "text", x = cMin-0.08*cRan, y = dMax+0.20*dRan, label = "niche themes", color = "gray50") +
#annotate(geom = "text", x = cMax+0.08*cRan, y = dMax+0.20*dRan, label = "motor themes", color = "gray50") +
#annotate(geom = "text", x = cMax+0.08*cRan, y = dMin-0.20*dRan, label = "basic themes", color = "gray50") +
#annotate(geom = "text", x = cMin-0.08*cRan, y = dMin-0.20*dRan, label = "up/down themes", color = "gray50") +
geom_point(alpha = .50) +
geom_text(aes(x = textX, label = top5), size = 3, fontface = "bold", color ="black") +
scale_x_continuous(name = "Centrality", breaks = NULL, limits = c(cMin-0.1*cRan,cMax+0.1*cRan)) +
scale_y_continuous(name = "Density", breaks = NULL, limits = c(dMin-0.2*dRan,dMax+0.2*dRan)) +
scale_color_manual(values = clustCol$hexCol) +
scale_size_continuous(range = c(10,30)) +
theme(
legend.position = "none"
)
themMapPlotMean
# save mean plots
ggsave(paste0(parentFolder, "/plots/thematicMaps/themMap_Mean_",fromYear,"to",toYear,".pdf"), themMapPlotMean,
width = 20, height = 20, units = "cm")
ggsave(paste0(parentFolder, "/plots/thematicMaps/themMap_Mean_",fromYear,"to",toYear,".png"), themMapPlotMean,
width = 20, height = 20, units = "cm", dpi = 600)
######################################
# coocMat <- matrix(data = NA, nrow = dim(occMat)[2], ncol = dim(occMat)[2])
#
# for (i in 1:dim(coocMat)[1]){
#   for (j in 1:dim(coocMat)[2]){
#     coocMat[i,j] <- sum(occMat[,i]*occMat[,j])
#   }
# }
#
# eiMat <- matrix(data = NA, nrow = 50, ncol = 50)
# asMat <- matrix(data = NA, nrow = 50, ncol = 50)
# for (rowI in 1:dim(asMat)[1]){
#   for (colI in 1:dim(asMat)[2]){
#     eiMat[rowI,colI] <- coocMat[rowI,colI]^2 / (totalKeyCounts[rowI]*totalKeyCounts[colI]) # Cobo (2011)
#     asMat[rowI,colI] <- 2*(sum(coocMat)/2)*coocMat[rowI,colI] /
#                         (sum(coocMat[rowI,])*sum(coocMat[,colI])) # van Eck & Waltmann (2014)
#   }
# }
#
# ###
#
#
#
# sumCooc <- sum(coocMat)
# rowSumsCooc <- rowSums(coocMat)
# colSumsCooc <- colSums(coocMat)
#
# asMatNew <- 2 * sumCooc * coocMat / (outer(rowSumsCooc, colSumsCooc))
# ###
#
#
# graphDF$centralityAS <- NA
# graphDF$densityAS <- NA
# graphDF$centralityEI <- NA
# graphDF$densityEI <- NA
#
# for (clustI in 1:dim(graphDF)[1]){
#   graphDF$centralityAS[clustI] <- 10 * sum(asMat[clusterIDs == clustI, clusterIDs != clustI])
#   graphDF$centralityEI[clustI] <- 10 * sum(eiMat[clusterIDs == clustI, clusterIDs != clustI])
#   graphDF$densityAS[clustI] <- 100 * sum(asMat[clusterIDs == clustI, clusterIDs = clustI]/2) / sum(clusterIDs == clustI)
#   graphDF$densityEI[clustI] <- 100 * sum(eiMat[clusterIDs == clustI, clusterIDs = clustI]/2) / sum(clusterIDs == clustI)
#   #graphDF$centrality[clustI] <- sum(asMat[clusterIDs == clustI, clusterIDs != clustI]^2)
#   #graphDF$density[clustI] <- (sum(asMat[clusterIDs == clustI, clusterIDs == clustI]) / 2) / (sum(clusterIDs == clustI)*(sum(clusterIDs == clustI)-1)/2)
# }
#
# flextable(graphDF)
#as.data.frame(as.matrix(network_TI))
#for (rowI in 1:50){
#  for (colI in 1:50){
#
#  }
#}
# for network analysis'
fromYear <- 1964
toYear <- 2023
nrNetworkItems <- NULL # how many items are included in the computation of the network (NULL = all available)
deleteSingleOccurrences <- TRUE # TRUE/FALSE: delete keywords that have a frequency of 1
#shortLabels <- TRUE # TRUE/FALSE: plot short labels (1st author + year)
# for plotting
nrItems2plot <- 50
minOccurences <- NULL # has priority over nrItems2plot if not set to NULL
plotTitle <- ""
mapType <- "auto" # type of map layout; choose between "auto", "cirlce", "sphere", "mds", "fruchterman", and "kamada"
clusterMethod <- "walktrap" # method to cluster key terms; choose between "none", "optimal", "louvain","leiden", "infomap","edge_betweenness","walktrap", "spinglass", "leading_eigen", "fast_greedy"
vosPath <- "C:\\VosViewerJar" # path of VOS Viewer needed if plotting should be done by this software
removeIsolates <- TRUE # remove keywords that do not co-occur sufficiently with other keywords
minEdgeStrength <- 5
#
showEntries <- 10
showEntriesQuant <- 50
fontSize = 12
### end header ###
# load packages
library(here)
library(bibliometrix)
library(dplyr)
library(tidyr)
library(flextable)
library(ggplot2)
# parent folder
parentFolder <- here()
# load  bibliometric data frame
filename <- paste0(parentFolder, "/rawData/allPapers.rds")
df <- readRDS(filename)
df$TI
df$TI[1]
is.element(df$TI,"AMBULATORY")
is.element("AMBULATORY",df$TI)
?substr
grepl("AMBULATORY",df$TI)
which(grepl("DIFFERENCES IN INDIVIDUAL",df$TI))
which(grepl("DIFFERENCES OF INDIVIDUAL",df$TI))
# load multi-word key terms to keep
terms2keep_TI <- read.table(paste0(parentFolder, "/terms/compoundTerms_TI_selected.txt"), sep = ";")[,1]
# load prepared lists of terms to delete and synonyms
terms2delete <- read.table(paste0(parentFolder, "/terms/terms2delete_TI.txt"), sep = ";")[,1]
synonyms <- read.table(paste0(parentFolder, "/terms/synonyms_TI.txt"), sep = ";")[,1]
### TEMPORARY FOR ADDING WORDS TO DELETE ###
#terms2delete <- c(terms2delete,
#                  "individual",
#                  "difference",
#                  "differences")
############################################
# extract keywords from article titles
df <- termExtraction(df, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = synonyms, verbose = TRUE)
# custom way to treat multi-word terms as one term, words joint by hyphen
replace2keep_TI <- gsub(" ",";",terms2keep_TI)
replaceWith_TI <- gsub(" ","-",terms2keep_TI)
for (i in 1:length(terms2keep_TI)){
df$TI_TM <- gsub(replace2keep_TI[i],replaceWith_TI[i],df$TI_TM)
}
which(grepl("INDIVIDUAL;DIFFERENCE",df$TI_TM))
df$TI[1]
df$TI[502]
df$TI[516]
df$TI[627]
df$TI[651]
tableTag(df, Tag = "TI_TM", remove.terms = terms2delete, synonyms = synonyms)
# for descriptive statistics: frequencies of the key terms from article titles
test <- tableTag(df, Tag = "TI_TM", remove.terms = terms2delete, synonyms = synonyms)
grepl("INDIVIDUAL DIFFERENCES",test)
which(grepl("INDIVIDUAL DIFFERENCES",test))
test
terms2keep_TI
grepl("INDIVIDUAL DIFFERENCES",terms2keep_TI)
which(grepl("INDIVIDUAL DIFFERENCES",terms2keep_TI))
df <- readRDS(filename)
# load multi-word key terms to keep
terms2keep_TI <- read.table(paste0(parentFolder, "/terms/compoundTerms_TI_selected.txt"), sep = ";")[,1]
# load prepared lists of terms to delete and synonyms
terms2delete <- read.table(paste0(parentFolder, "/terms/terms2delete_TI.txt"), sep = ";")[,1]
synonyms <- read.table(paste0(parentFolder, "/terms/synonyms_TI.txt"), sep = ";")[,1]
# extract keywords from article titles
df <- termExtraction(df, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = synonyms, verbose = TRUE)
# custom way to treat multi-word terms as one term, words joint by hyphen
replace2keep_TI <- gsub(" ",";",terms2keep_TI)
replaceWith_TI <- gsub(" ","-",terms2keep_TI)
for (i in 1:length(terms2keep_TI)){
df$TI_TM <- gsub(replace2keep_TI[i],replaceWith_TI[i],df$TI_TM)
}
terms2keep_TI
replace2keep_TI
replaceWith_TI
?termExtraction
df$TI[1]
terms2keep_TI
df <- readRDS(filename)
# load multi-word key terms to keep
terms2keep_TI <- read.table(paste0(parentFolder, "/terms/compoundTerms_TI_selected.txt"), sep = ";")[,1]
# load prepared lists of terms to delete and synonyms
terms2delete <- read.table(paste0(parentFolder, "/terms/terms2delete_TI.txt"), sep = ";")[,1]
synonyms <- read.table(paste0(parentFolder, "/terms/synonyms_TI.txt"), sep = ";")[,1]
terms2delete
# custom way to treat multi-word terms as one term, words joint by hyphen
replace2keep_TI <- gsub(" "," ",terms2keep_TI)
replaceWith_TI <- gsub(" ","-",terms2keep_TI)
# custom way to treat multi-word terms as one term, words joint by hyphen
replace2keep_TI <- gsub(" "," ",terms2keep_TI)
replaceWith_TI <- gsub(" ","-",terms2keep_TI)
for (i in 1:length(terms2keep_TI)){
df$TI <- gsub(replace2keep_TI[i],replaceWith_TI[i],df$TI)
}
df$TI[1]
df$TI[2]
# extract keywords from article titles
df <- termExtraction(df, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = synonyms, verbose = TRUE)
# for descriptive statistics: frequencies of the key terms from article titles
tableTag(df, Tag = "TI_TM", remove.terms = terms2delete, synonyms = synonyms)
testdf <- df[1:10,]
synonyms
testdf <- df[1:5,]
testdf$TI <- c("test abc", "tests abc", "test-test def", "testsss klo", "noch was")
testdf$TI
testdf
View(testdf)
testdf$TI
testdf$TI_TM
str(testdf$TI_TM)
str(testdf$TI)
testdf$TI
testdf$TI_TM
testdf$TI <- rbind("test abc", "tests abc", "test-test def", "testsss klo", "noch was")
testdf$TI
testdf$TI_TM
testdf <- df[1:5,]
testdf$TI[1] <- "test abc"
testdf$TI[2] <- "tests abc"
testdf$TI[3] <- "test-test def"
testdf$TI[4] <- "testsss klo"
testdf$TI[5] <- "noch was"
testdf$TI
testdf
synonyms
testdf <- df[1:5,]
testsyn <- c("test; tests", "test; testsss")
# extract keywords from article titles
df <- termExtraction(dfdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = testsyn, verbose = TRUE)
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = testsyn, verbose = TRUE)
testdf$TI[1] <- "test abc"
testdf$TI[2] <- "tests abc"
testdf$TI[3] <- "test-test def"
testdf$TI[4] <- "testsss klo"
testdf$TI[5] <- "noch was"
testdf$TI
testdf$TI[2]
source("C:/Users/Christian Panitz/Dropbox/BibAnalyses_Psychophys_R/scripts/CopyOf07_titleTrends.R", echo=TRUE)
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = testsyn, verbose = TRUE)
testdf <- df[1:5,]
testdf$TI[1] <- "test abc"
testdf$TI[2] <- "tests abc"
testdf$TI[3] <- "test-test def"
testdf$TI[4] <- "testsss klo"
testdf$TI[5] <- "noch was"
testsyn <- c("test; tests", "test; testsss")
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = terms2delete, synonyms = testsyn, verbose = TRUE)
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = NA, synonyms = testsyn, verbose = TRUE)
terms2delete
testrem <- "test"
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = NA, synonyms = testsyn, verbose = TRUE)
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = testrem, synonyms = testsyn, verbose = TRUE)
# extract keywords from article titles
df <- termExtraction(testdf, Field = "TI", ngrams = 1, remove.numbers = FALSE,
remove.terms = testrem, synonyms = NA, verbose = TRUE)
?cocMatrix
??cocMatrix
citation("bibliometrix")
environment()
environment("bibliometrix")
environment("packages")
citation()
version()
R.version
citation("tidytext")
